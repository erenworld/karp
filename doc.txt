// The algorithm behind the parseExpression method and its combination of parsing functions
// and precedences is fully described by Vaughan Pratt in his “Top Down Operator Precedence”
// paper. But there are differences between his and our implementation

// Suppose we’re parsing the following expression statement:
// 1 + 2 + 3;

// What we want is an AST that (serialized as a string) looks like this:
// ((1 + 2) + 3)

// parseExpressionStatement calls parseExpression(LOWEST). The p.curToken and p.peekToken are
// the 1 and the first +:

// 1 is p.curToken 
// + is p.peekToken

// The first thing parseExpression then does is to check whether there is a prefixParseFn associated
// with the current p.curToken.Type, which is a token.INT. And, yes, there is: parseIntegerLiteral.
// So it calls parseIntegerLiteral, which returns an *ast.IntegerLiteral. parseExpression assigns
// this to leftExp.

// Then comes the new for-loop in parseExpression. Its condition evaluates to true:
// for !p.peekTokenIs(token.SEMICOLON) && precedence < p.peekPrecedence() {
// [...]
// }

// parseExpression executes the body of the loop, which
// looks like this:
// 		infix := p.infixParseFns[p.peekToken.Type]
// 		if infix == nil {
// 			return leftExp
// 		}
// 		p.nextToken()
// 		leftExp = infix(leftExp)


// Now back in parseInfixExpression the return-value of parseExpression is assigned to the Right
// field of the newly constructed *ast.InfixExpression. 

// ast.infixExpression -> ast.integerIntegral -> 1
// ast.infixExpression -> ast.integerIntegral -> 2


function litterals
fn <parameters> <block statement>

call expression
<expression>(<comma separated expressions>)
add(2, 3)

Evaluation. The E in REPL and the last thing an interpreter has to do
when processing source code. This is where code becomes meaningful. Without evaluation an
expression like 1 + 2 is just a series of characters, tokens, or a tree structure that represents this
expression. It doesn’t mean anything. Evaluated, of course, 1 + 2 becomes 3. 5 > 1 becomes
true, 5 < 1 becomes false and puts("Hello World!") becomes the friendly message we all know.

1. “tree-walking interpreters” -> slowest
the most obvious and classical choice of what to do with the AST is to just
interpret it. Traverse the AST, visit each node and do what the node signifies: print a string, add
two numbers, execute a function’s body - all on the fly. Interpreters working this way are called
“tree-walking interpreters” and are the archetype of interpreters. Sometimes their evaluation
step is preceded by small optimizations that rewrite the AST (e.g. remove unused variable
bindings) or convert it into another intermediate representation (IR) that’s more suitable for
recursive and repeated evaluation.

2. Other interpreters also traverse the AST, but instead of interpreting the AST itself they first
convert it to bytecode. Bytecode is another IR of the AST and a really dense one at that. The
exact format and of which opcodes (the instructions that make up the bytecode) it’s composed
of varies and depends on the guest and host programming languages. 

e.g: Now the Ruby interpreter parses source code, builds an AST and then compiles this AST into
bytecode, which gets then executed in a virtual machine.
